[
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "FIN 5210: Investment Analysis (EMBA/MBA/MSc, Fall 2021/2022/2023)\nFIN 5230: ESG Investing (MSc, Spring 2021/2023)\nFIN 6910: Green Finance and Capital Markets (MSc, Fall 2023)\n\n\n\nFIN 2203: Fundamentals of Business Finance (UG, Fall 2020)\nFIN 3303: Intermediate Corporate Finance (UG, Fall 2020)\nFIN 4803: Quantitative Trading (UG, Spring 2022)"
  },
  {
    "objectID": "teaching.html#teaching-assistant-hkust",
    "href": "teaching.html#teaching-assistant-hkust",
    "title": "Teaching",
    "section": "",
    "text": "FIN 5210: Investment Analysis (EMBA/MBA/MSc, Fall 2021/2022/2023)\nFIN 5230: ESG Investing (MSc, Spring 2021/2023)\nFIN 6910: Green Finance and Capital Markets (MSc, Fall 2023)\n\n\n\nFIN 2203: Fundamentals of Business Finance (UG, Fall 2020)\nFIN 3303: Intermediate Corporate Finance (UG, Fall 2020)\nFIN 4803: Quantitative Trading (UG, Spring 2022)"
  },
  {
    "objectID": "teaching/codesets/FINA6900A_Regression.html",
    "href": "teaching/codesets/FINA6900A_Regression.html",
    "title": "Linear Regression Demo (Chapter 3 - ISLR)",
    "section": "",
    "text": "Load Dataset: advertising\nLinear Regression\n\nSimple linear regressions\nMultiple linear regressions\n\n\n\n## perform imports and set-up\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import axes3d\nimport seaborn as sns\n\nfrom sklearn.preprocessing import scale\nimport sklearn.linear_model as skl_lm\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n%matplotlib inline\nplt.style.use('ggplot') # emulate pretty r-style plots\n\n/Users/yuany/anaconda3/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n  from pandas.core import datetools\n\n\nLoad Datasets Datasets available on http://www-bcf.usc.edu/~gareth/ISL/data.html\n\nadvertising = pd.read_csv('../data/Advertising.csv', usecols=[1,2,3,4])\nadvertising\n\n\n\n\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\n0\n230.1\n37.8\n69.2\n22.1\n\n\n1\n44.5\n39.3\n45.1\n10.4\n\n\n2\n17.2\n45.9\n69.3\n9.3\n\n\n3\n151.5\n41.3\n58.5\n18.5\n\n\n4\n180.8\n10.8\n58.4\n12.9\n\n\n5\n8.7\n48.9\n75.0\n7.2\n\n\n6\n57.5\n32.8\n23.5\n11.8\n\n\n7\n120.2\n19.6\n11.6\n13.2\n\n\n8\n8.6\n2.1\n1.0\n4.8\n\n\n9\n199.8\n2.6\n21.2\n10.6\n\n\n10\n66.1\n5.8\n24.2\n8.6\n\n\n11\n214.7\n24.0\n4.0\n17.4\n\n\n12\n23.8\n35.1\n65.9\n9.2\n\n\n13\n97.5\n7.6\n7.2\n9.7\n\n\n14\n204.1\n32.9\n46.0\n19.0\n\n\n15\n195.4\n47.7\n52.9\n22.4\n\n\n16\n67.8\n36.6\n114.0\n12.5\n\n\n17\n281.4\n39.6\n55.8\n24.4\n\n\n18\n69.2\n20.5\n18.3\n11.3\n\n\n19\n147.3\n23.9\n19.1\n14.6\n\n\n20\n218.4\n27.7\n53.4\n18.0\n\n\n21\n237.4\n5.1\n23.5\n12.5\n\n\n22\n13.2\n15.9\n49.6\n5.6\n\n\n23\n228.3\n16.9\n26.2\n15.5\n\n\n24\n62.3\n12.6\n18.3\n9.7\n\n\n25\n262.9\n3.5\n19.5\n12.0\n\n\n26\n142.9\n29.3\n12.6\n15.0\n\n\n27\n240.1\n16.7\n22.9\n15.9\n\n\n28\n248.8\n27.1\n22.9\n18.9\n\n\n29\n70.6\n16.0\n40.8\n10.5\n\n\n...\n...\n...\n...\n...\n\n\n170\n50.0\n11.6\n18.4\n8.4\n\n\n171\n164.5\n20.9\n47.4\n14.5\n\n\n172\n19.6\n20.1\n17.0\n7.6\n\n\n173\n168.4\n7.1\n12.8\n11.7\n\n\n174\n222.4\n3.4\n13.1\n11.5\n\n\n175\n276.9\n48.9\n41.8\n27.0\n\n\n176\n248.4\n30.2\n20.3\n20.2\n\n\n177\n170.2\n7.8\n35.2\n11.7\n\n\n178\n276.7\n2.3\n23.7\n11.8\n\n\n179\n165.6\n10.0\n17.6\n12.6\n\n\n180\n156.6\n2.6\n8.3\n10.5\n\n\n181\n218.5\n5.4\n27.4\n12.2\n\n\n182\n56.2\n5.7\n29.7\n8.7\n\n\n183\n287.6\n43.0\n71.8\n26.2\n\n\n184\n253.8\n21.3\n30.0\n17.6\n\n\n185\n205.0\n45.1\n19.6\n22.6\n\n\n186\n139.5\n2.1\n26.6\n10.3\n\n\n187\n191.1\n28.7\n18.2\n17.3\n\n\n188\n286.0\n13.9\n3.7\n15.9\n\n\n189\n18.7\n12.1\n23.4\n6.7\n\n\n190\n39.5\n41.1\n5.8\n10.8\n\n\n191\n75.5\n10.8\n6.0\n9.9\n\n\n192\n17.2\n4.1\n31.6\n5.9\n\n\n193\n166.8\n42.0\n3.6\n19.6\n\n\n194\n149.7\n35.6\n6.0\n17.3\n\n\n195\n38.2\n3.7\n13.8\n7.6\n\n\n196\n94.2\n4.9\n8.1\n9.7\n\n\n197\n177.0\n9.3\n6.4\n12.8\n\n\n198\n283.6\n42.0\n66.2\n25.5\n\n\n199\n232.1\n8.6\n8.7\n13.4\n\n\n\n\n200 rows × 4 columns\n\n\n\n\nPairwise scatter plot\n\nsns.pairplot(advertising[['Sales','TV','Radio','Newspaper']]);\n\n\n\n\n\n\n\n\n\n\nSimple linear regressions\n\nsns.regplot(advertising.TV, advertising.Sales, order=1, ci=None, scatter_kws={'color':'r', 's':9})\nplt.xlim(-10,310)\nplt.ylim(ymin=0);\n\n\n\n\n\n\n\n\nParameter estimation and uncertainty\n\nest = smf.ols('Sales ~ TV', advertising).fit()\nest.summary().tables[1]\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n7.0326\n0.458\n15.360\n0.000\n6.130\n7.935\n\n\nTV\n0.0475\n0.003\n17.668\n0.000\n0.042\n0.053\n\n\n\n\n\n\nsns.regplot(advertising.Radio, advertising.Sales, order=1, ci=None, scatter_kws={'color':'r', 's':9})\nplt.xlim(-1,50)\nplt.ylim(ymin=0);\n\n\n\n\n\n\n\n\n\nest = smf.ols('Sales ~ Radio', advertising).fit()\nest.summary().tables[1]\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n9.3116\n0.563\n16.542\n0.000\n8.202\n10.422\n\n\nRadio\n0.2025\n0.020\n9.921\n0.000\n0.162\n0.243\n\n\n\n\n\n\nsns.regplot(advertising.Newspaper, advertising.Sales, order=1, ci=None, scatter_kws={'color':'r', 's':9})\nplt.xlim(-1,50)\nplt.ylim(ymin=0);\n\n\n\n\n\n\n\n\n\nest = smf.ols('Sales ~ Newspaper', advertising).fit()\nest.summary().tables[1]\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n12.3514\n0.621\n19.876\n0.000\n11.126\n13.577\n\n\nNewspaper\n0.0547\n0.017\n3.300\n0.001\n0.022\n0.087\n\n\n\n\n\n\n\nMultiple Linear Regressions\n\nest = smf.ols('Sales ~ TV + Radio + Newspaper', advertising).fit()\nest.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nSales\nR-squared:\n0.897\n\n\nModel:\nOLS\nAdj. R-squared:\n0.896\n\n\nMethod:\nLeast Squares\nF-statistic:\n570.3\n\n\nDate:\nSat, 29 Feb 2020\nProb (F-statistic):\n1.58e-96\n\n\nTime:\n21:33:59\nLog-Likelihood:\n-386.18\n\n\nNo. Observations:\n200\nAIC:\n780.4\n\n\nDf Residuals:\n196\nBIC:\n793.6\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2.9389\n0.312\n9.422\n0.000\n2.324\n3.554\n\n\nTV\n0.0458\n0.001\n32.809\n0.000\n0.043\n0.049\n\n\nRadio\n0.1885\n0.009\n21.893\n0.000\n0.172\n0.206\n\n\nNewspaper\n-0.0010\n0.006\n-0.177\n0.860\n-0.013\n0.011\n\n\n\n\n\n\n\n\nOmnibus:\n60.414\nDurbin-Watson:\n2.084\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n151.241\n\n\nSkew:\n-1.327\nProb(JB):\n1.44e-33\n\n\nKurtosis:\n6.332\nCond. No.\n454.\n\n\n\n\n\nNotice that the p-values of TV and Radio coefficients are small, while that of Newspaper is large (0.860). It suggests to drop the variable Newspaper in the model.\n\nest = smf.ols('Sales ~ TV + Radio', advertising).fit()\nest.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nSales\nR-squared:\n0.897\n\n\nModel:\nOLS\nAdj. R-squared:\n0.896\n\n\nMethod:\nLeast Squares\nF-statistic:\n859.6\n\n\nDate:\nSat, 29 Feb 2020\nProb (F-statistic):\n4.83e-98\n\n\nTime:\n21:34:07\nLog-Likelihood:\n-386.20\n\n\nNo. Observations:\n200\nAIC:\n778.4\n\n\nDf Residuals:\n197\nBIC:\n788.3\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2.9211\n0.294\n9.919\n0.000\n2.340\n3.502\n\n\nTV\n0.0458\n0.001\n32.909\n0.000\n0.043\n0.048\n\n\nRadio\n0.1880\n0.008\n23.382\n0.000\n0.172\n0.204\n\n\n\n\n\n\n\n\nOmnibus:\n60.022\nDurbin-Watson:\n2.081\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n148.679\n\n\nSkew:\n-1.323\nProb(JB):\n5.19e-33\n\n\nKurtosis:\n6.292\nCond. No.\n425.\n\n\n\n\n\nNotice that this two-variable model has R-squared=0.897, the same as three-variable model in training error."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Aggregators\nBrendan Price with resources for PhD students\n\n\nLearning Your Production Function\n\nLife Advice\n\n\nGeneral Pipline\n\n\nGrad Life\n\n\n\nWorking on Projects\n\nGenerating Ideas\n\n\nLiterature Review\n\n\nCoding\n\nStata & R with a side-by-side comparison\nSean Higgin’s R guide\nDan Sullivan’s best practices for coding\n\n\n\nData\n\n\nProject Management\n\n\n\nWriting & Presenting Your Work\n\nSlides\nPaul Goldsmith-Pinkham on presentations in beamer\n\n\nAttending\n\n\nPresenting\n\n\nManuscript\n\n\nFigures & Tables\n\n\n\n\n\n\nTeaching Notes\n\nPhD Notes\nJesús Fernández-Villaverde on computation, macroeconomics, and economic history\n\n\nUG Notes\nRaj Chetty on Using Big Data to Solve Economic and Social Problems\n\n\n\nRecommendations\n\nEcon & Finance Books\n\n\nGame\n\n\nPodcasts & Blogs\n\n\nEcon & Finance Books"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Zhuang CHU (储庄)",
    "section": "",
    "text": "I am a Ph.D. candidate in Finance at Hong Kong University of Science and Technology (HKUST). \nMy research interests lie in Sustainable Finance and Empirical Corporate Finance. In particular, I focus on studying the impact of financial and political frictions on the resource allocation efficiency during the low-carbon transition.\nI am on the job market for 2024-2025. You can find my CV here.\nEmail: zchu@connect.ust.hk"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "How Does Political Leaning Affect Green Investment Efficiency? Evidence from US Solar Power Plants, Job Market Paper, 2024\n\nConferences & Seminars: HKUST Brown Bag, 2nd HEC-HKUST Sustainable Finance Workshop\nAbstract: This study uses solar power plants as a laboratory to examine the impact of political leaning on the efficiency of green infrastructure in the US. Using plant-level data and exploiting policy discontinuities at state borders, I find that Democratic-leaning states have more but less efficient solar plants, while Republican-leaning states have fewer but more efficient ones. The efficiency gap is explained by the in-elasticity of solar plant size to installation cost in Democratic-leaning states. Discontinuity and missing mass in size distribution suggest that this inefficiency stems from developers’ self-selection due to policy distortion. I confirm this mechanism through a difference-in-differences design, leveraging an unexpected federal subsidy policy expansion in 2016. The findings highlight how politically-driven climate policies may lead to unintended efficiency loss.\n\nTransition Risk under Capital Misallocation: The Deployment of Solar Power Plants in China, with Zhanhui Chen, 2024\n\nConferences & Seminar: NBER-SAIF Climate Finance and the Sustainable Energy Transition Meeting, 2024 China Financial Research Conference, HKUST Brown Bag*, Southern University of Science and Technology*, Renmin University of China\nAbstract: This paper examines the financial impacts of transition risk on firms and aggregate economy through the deployment of solar power plants (SPP) in China. We found that more SPP were deployed in areas with lower solar radiation and negatively affected the local economy. Cities with SPP experienced a lower local GDP growth of approximately 0.8-1.8%. At the firm level, SPP deployment decreased corporate investment and debt financing, and increased financing costs in other sectors. These effects were more pronounced for private firms, firms relying on external financing or productive firms. The crowding-out effect under capital misallocation drives our findings.\n\nES Performance and Voting Premium: Evidence from Option Prices, 2022\n\nConferences & Seminar: Yonsei University\nAbstract: This paper examines the impact of environmental and social (ES) performance on shareholder voting rights value, estimated using option prices. Results show companies with higher ES performance have lower voting premiums, primarily driven by the product-related dimension. Exploiting negative ES news shocks, the study confirms poor ES performance increases voting premium around shareholder meetings. This suggests voting premium reflects managerial inefficiencies, consistent with the view that negative news can be used to exert disciplinary pressure during ES meetings. However, the research finds neither pronounced dynamics reflecting growing ES interest nor increased voting premium after negative ES news. These findings contribute to understanding how ES performance influences corporate governance and shareholder value, offering insights for investors, managers, and policymakers in sustainable finance.\n\nThe Impact of Nuclear Energy on Social Welfare and the Future of Renewable Energy: Episode from South Korea, with Kwangwon Ahn, and Minhyuk Jeong, 2024\n\nR&R in Humanities and Social Sciences Communications"
  },
  {
    "objectID": "research.html#working-papers",
    "href": "research.html#working-papers",
    "title": "Research",
    "section": "",
    "text": "How Does Political Leaning Affect Green Investment Efficiency? Evidence from US Solar Power Plants, Job Market Paper, 2024\n\nConferences & Seminars: HKUST Brown Bag, 2nd HEC-HKUST Sustainable Finance Workshop\nAbstract: This study uses solar power plants as a laboratory to examine the impact of political leaning on the efficiency of green infrastructure in the US. Using plant-level data and exploiting policy discontinuities at state borders, I find that Democratic-leaning states have more but less efficient solar plants, while Republican-leaning states have fewer but more efficient ones. The efficiency gap is explained by the in-elasticity of solar plant size to installation cost in Democratic-leaning states. Discontinuity and missing mass in size distribution suggest that this inefficiency stems from developers’ self-selection due to policy distortion. I confirm this mechanism through a difference-in-differences design, leveraging an unexpected federal subsidy policy expansion in 2016. The findings highlight how politically-driven climate policies may lead to unintended efficiency loss.\n\nTransition Risk under Capital Misallocation: The Deployment of Solar Power Plants in China, with Zhanhui Chen, 2024\n\nConferences & Seminar: NBER-SAIF Climate Finance and the Sustainable Energy Transition Meeting, 2024 China Financial Research Conference, HKUST Brown Bag*, Southern University of Science and Technology*, Renmin University of China\nAbstract: This paper examines the financial impacts of transition risk on firms and aggregate economy through the deployment of solar power plants (SPP) in China. We found that more SPP were deployed in areas with lower solar radiation and negatively affected the local economy. Cities with SPP experienced a lower local GDP growth of approximately 0.8-1.8%. At the firm level, SPP deployment decreased corporate investment and debt financing, and increased financing costs in other sectors. These effects were more pronounced for private firms, firms relying on external financing or productive firms. The crowding-out effect under capital misallocation drives our findings.\n\nES Performance and Voting Premium: Evidence from Option Prices, 2022\n\nConferences & Seminar: Yonsei University\nAbstract: This paper examines the impact of environmental and social (ES) performance on shareholder voting rights value, estimated using option prices. Results show companies with higher ES performance have lower voting premiums, primarily driven by the product-related dimension. Exploiting negative ES news shocks, the study confirms poor ES performance increases voting premium around shareholder meetings. This suggests voting premium reflects managerial inefficiencies, consistent with the view that negative news can be used to exert disciplinary pressure during ES meetings. However, the research finds neither pronounced dynamics reflecting growing ES interest nor increased voting premium after negative ES news. These findings contribute to understanding how ES performance influences corporate governance and shareholder value, offering insights for investors, managers, and policymakers in sustainable finance.\n\nThe Impact of Nuclear Energy on Social Welfare and the Future of Renewable Energy: Episode from South Korea, with Kwangwon Ahn, and Minhyuk Jeong, 2024\n\nR&R in Humanities and Social Sciences Communications"
  },
  {
    "objectID": "research.html#publications",
    "href": "research.html#publications",
    "title": "Research",
    "section": "Publications",
    "text": "Publications\nEffects of renewable energy use in the energy mix on social welfare,  with Kwangwon Ahn, Daeyong Lee, Published at Energy Economics, 2021\nModeling GDP Fluctuations with Agent-based Model, with Kwangwon Ahn, Changyong Ha, Biao Yang, Published at Physica A: Statistical Mechanics and its Applications, 2018"
  },
  {
    "objectID": "research.html#work-in-progress",
    "href": "research.html#work-in-progress",
    "title": "Research",
    "section": "Work in progress",
    "text": "Work in progress\nPolitical Polarization and Households’ Climate Adoption, with Zhanhui Chen"
  },
  {
    "objectID": "teaching/codesets/FINA6900A_Classification.html",
    "href": "teaching/codesets/FINA6900A_Classification.html",
    "title": "Chapter 4 - Classification",
    "section": "",
    "text": "Load dataset\nThe Default data set\n4.3 Logistic Regression\n4.4 Linear Discriminant Analysis"
  },
  {
    "objectID": "teaching/codesets/FINA6900A_Classification.html#logistic-regression",
    "href": "teaching/codesets/FINA6900A_Classification.html#logistic-regression",
    "title": "Chapter 4 - Classification",
    "section": "4.3 Logistic Regression",
    "text": "4.3 Logistic Regression\n\nFigure 4.2\n\nX_train = df.balance.values.reshape(-1,1) \ny = df.default2\n\n# Create array of test data. Calculate the classification probability\n# and predicted classification.\nX_test = np.arange(df.balance.min(), df.balance.max()).reshape(-1,1)\n\nclf = skl_lm.LogisticRegression(solver='newton-cg')\nclf.fit(X_train,y)\nprob = clf.predict_proba(X_test)\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5))\n# Left plot: Linear Regression\nsns.regplot(df.balance, df.default2, order=1, ci=None,\n            scatter_kws={'color':'orange'},\n            line_kws={'color':'lightblue', 'lw':2}, ax=ax1)\n# Right plot: Logistic Regression\nax2.scatter(X_train, y, color='orange')\nax2.plot(X_test, prob[:,1], color='lightblue')\n\nfor ax in fig.axes:\n    ax.hlines(1, xmin=ax.xaxis.get_data_interval()[0],\n              xmax=ax.xaxis.get_data_interval()[1], linestyles='dashed', lw=1)\n    ax.hlines(0, xmin=ax.xaxis.get_data_interval()[0],\n              xmax=ax.xaxis.get_data_interval()[1], linestyles='dashed', lw=1)\n    ax.set_ylabel('Probability of default')\n    ax.set_xlabel('Balance')\n    ax.set_yticks([0, 0.25, 0.5, 0.75, 1.])\n    ax.set_xlim(xmin=-100)\n\n\n\n\n\n\n\n\n\n\nTable 4.1\n\ny = df.default2\n\n\nscikit-learn\n\n# Using newton-cg solver, the coefficients are equal/closest to the ones in the book. \n# I do not know the details on the differences between the solvers.\nclf = skl_lm.LogisticRegression(solver='newton-cg')\nX_train = df.balance.values.reshape(-1,1)\nclf.fit(X_train,y)\nprint(clf)\nprint('classes: ',clf.classes_)\nprint('coefficients: ',clf.coef_)\nprint('intercept :', clf.intercept_)\n\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='newton-cg', tol=0.0001,\n          verbose=0, warm_start=False)\nclasses:  [0 1]\ncoefficients:  [[ 0.00549891]]\nintercept : [-10.65131887]\n\n\n\n\nstatsmodels\n\nX_train = sm.add_constant(df.balance)\nest = smf.Logit(y.ravel(), X_train).fit()\nest.summary().tables[1]\n\nOptimization terminated successfully.\n         Current function value: 0.079823\n         Iterations 10\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-10.6513\n0.361\n-29.491\n0.000\n-11.359\n-9.943\n\n\nbalance\n0.0055\n0.000\n24.952\n0.000\n0.005\n0.006\n\n\n\n\n\n\n\n\nTable 4.2\n\nX_train = sm.add_constant(df.student2)\ny = df.default2\n\nest = smf.Logit(y, X_train).fit()\nest.summary().tables[1]\n\nOptimization terminated successfully.\n         Current function value: 0.145434\n         Iterations 7\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-3.5041\n0.071\n-49.554\n0.000\n-3.643\n-3.366\n\n\nstudent2\n0.4049\n0.115\n3.520\n0.000\n0.179\n0.630\n\n\n\n\n\n\n\nTable 4.3 - Multiple Logistic Regression\n\nX_train = sm.add_constant(df[['balance', 'income', 'student2']])\nest = smf.Logit(y, X_train).fit()\nest.summary().tables[1]\n\nOptimization terminated successfully.\n         Current function value: 0.078577\n         Iterations 10\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-10.8690\n0.492\n-22.079\n0.000\n-11.834\n-9.904\n\n\nbalance\n0.0057\n0.000\n24.737\n0.000\n0.005\n0.006\n\n\nincome\n3.033e-06\n8.2e-06\n0.370\n0.712\n-1.3e-05\n1.91e-05\n\n\nstudent2\n-0.6468\n0.236\n-2.738\n0.006\n-1.110\n-0.184\n\n\n\n\n\nwhere we can see the P-value of income coefficient is big (0.712). How about dropping it in the model?\n\nX_train = sm.add_constant(df[['balance', 'student2']])\nest1 = smf.Logit(y, X_train).fit()\nest1.summary().tables[1]\n\nOptimization terminated successfully.\n         Current function value: 0.078584\n         Iterations 10\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-10.7495\n0.369\n-29.115\n0.000\n-11.473\n-10.026\n\n\nbalance\n0.0057\n0.000\n24.748\n0.000\n0.005\n0.006\n\n\nstudent2\n-0.7149\n0.148\n-4.846\n0.000\n-1.004\n-0.426\n\n\n\n\n\nIn all the experiments above, student[Yes] has a negative coefficient. Does it mean that being a student will reduce the default risk?\nThe following experiment will explore it.\n\n\nFigure 4.3 - Confounding\n\n# balance and default vectors for students\nX_train = df[df.student == 'Yes'].balance.values.reshape(df[df.student == 'Yes'].balance.size,1) \ny = df[df.student == 'Yes'].default2\n\n# balance and default vectors for non-students\nX_train2 = df[df.student == 'No'].balance.values.reshape(df[df.student == 'No'].balance.size,1) \ny2 = df[df.student == 'No'].default2\n\n# Vector with balance values for plotting\nX_test = np.arange(df.balance.min(), df.balance.max()).reshape(-1,1)\n\nclf = skl_lm.LogisticRegression(solver='newton-cg')\nclf2 = skl_lm.LogisticRegression(solver='newton-cg')\n\nclf.fit(X_train,y)\nclf2.fit(X_train2,y2)\n\nprob = clf.predict_proba(X_test)\nprob2 = clf2.predict_proba(X_test)\n\n\ndf.groupby(['student','default']).size().unstack('default')\n\ndefault    No  Yes\nstudent           \nNo       6850  206\nYes      2817  127\n\n\n\n# creating plot\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,5))\n\n# Left plot\nax1.plot(X_test, pd.DataFrame(prob)[1], color='orange', label='Student')\nax1.plot(X_test, pd.DataFrame(prob2)[1], color='lightblue', label='Non-student')\nax1.hlines(127/2817, colors='orange', label='Overall Student',\n           xmin=ax1.xaxis.get_data_interval()[0],\n           xmax=ax1.xaxis.get_data_interval()[1], linestyles='dashed')\nax1.hlines(206/6850, colors='lightblue', label='Overall Non-Student',\n           xmin=ax1.xaxis.get_data_interval()[0],\n           xmax=ax1.xaxis.get_data_interval()[1], linestyles='dashed')\nax1.set_ylabel('Default Rate')\nax1.set_xlabel('Credit Card Balance')\nax1.set_yticks([0, 0.2, 0.4, 0.6, 0.8, 1.])\nax1.set_xlim(450,2500)\nax1.legend(loc=2)\n\nc_palette = {'No':'lightblue', 'Yes':'orange'}\n\n# Right plot\nsns.boxplot('student', 'balance', data=df, orient='v', ax=ax2,  palette=c_palette);\n\n\n\n\n\n\n\n\nFrom the figures above, the group of students has a higher mean credit balance than the others. A higher balance will lead to a higher default rate. Therefore, being student and balance are highly correlated predictors. When such correlated predictors exist, one has to be careful on the interpretation of coefficients which might not be true in reality.\nHere we have seen the phenomenon of confounding. In statistics, a confounder (also confounding variable, confounding factor, or lurking variable) is a variable that influences both the response and independent variable, causing a spurious association. Being a student is a confounder that influence both the independent variable balance and response default. Confounding is a causal concept, and as such, cannot be described in terms of correlations or associations."
  },
  {
    "objectID": "teaching/codesets/FINA6900A_Classification.html#linear-discriminant-analysis",
    "href": "teaching/codesets/FINA6900A_Classification.html#linear-discriminant-analysis",
    "title": "Chapter 4 - Classification",
    "section": "4.4 Linear Discriminant Analysis",
    "text": "4.4 Linear Discriminant Analysis\n\nTable 4.4\n\nX = df[['balance', 'income', 'student2']].as_matrix()\ny = df.default2.as_matrix()\n\nlda = LinearDiscriminantAnalysis(solver='svd')\ny_pred = lda.fit(X, y).predict(X)\n\ndf_ = pd.DataFrame({'True default status': y,\n                    'Predicted default status': y_pred})\ndf_.replace(to_replace={0:'No', 1:'Yes'}, inplace=True)\n\ndf_.groupby(['Predicted default status','True default status']).size().unstack('True default status')\n\nTrue default status         No  Yes\nPredicted default status           \nNo                        9645  254\nYes                         22   79\n\n\nThat stands for [[‘TN’, ‘FN’],[‘FP’,‘TP’]].\n\n# Show confusion_matrix \ncm = confusion_matrix(y, y_pred)\nprint(cm.T)\n\n[[9645  254]\n [  22   79]]\n\n\n\ndef plot_confusion_matrix(cm, title='Confusion matrix (Normalized)',\n                          cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Normalized confusion matrix')\n    plt.colorbar()\n    plt.tight_layout()\n    plt.xlabel('True label',rotation='horizontal', ha='right')\n    plt.ylabel('Predicted label')\n    plt.show()\n\n\ncm = confusion_matrix(y, y_pred)\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nplot_confusion_matrix(cm_normalized.T)\n\n\n\n\n\n\n\n\n\nprint(classification_report(y, y_pred, target_names=['No', 'Yes'], digits=3))\n\n             precision    recall  f1-score   support\n\n         No      0.974     0.998     0.986      9667\n        Yes      0.782     0.237     0.364       333\n\navg / total      0.968     0.972     0.965     10000\n\n\n\nwhere - precision = 1 - false discovery rate = TP/P* = TP/(TP+FP) - recall = sensitivity = power = 1 - Type II error = TP/P = TP/(TP+FN) - f1-score = 2((precisionrecall)/(precision+recall))\n\n\nTable 4.5\nInstead of using the probability of 50% as decision boundary, we say that a probability of default of 20% is to be classified as ‘Yes’.\n\ndecision_prob = 0.2\ny_prob = lda.fit(X, y).predict_proba(X)\n\ndf_ = pd.DataFrame({'True default status': y,\n                    'Predicted default status': y_prob[:,1] &gt; decision_prob})\ndf_.replace(to_replace={0:'No', 1:'Yes', 'True':'Yes', 'False':'No'}, inplace=True)\n\ndf_.groupby(['Predicted default status','True default status']).size().unstack('True default status')\n\nTrue default status         No  Yes\nPredicted default status           \nNo                        9435  140\nYes                        232  193\n\n\n\n# Check confusion matrix \ncm = confusion_matrix(y, y_prob[:,1] &gt; decision_prob)\nprint(cm.T)\n\n[[9435  140]\n [ 232  193]]\n\n\n\ncm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\nplot_confusion_matrix(cm_normalized.T)\n\n\n\n\n\n\n\n\n\nprint(classification_report(y, y_prob[:,1] &gt; decision_prob, target_names=['No', 'Yes'], digits=3))\n\n             precision    recall  f1-score   support\n\n         No      0.985     0.976     0.981      9667\n        Yes      0.454     0.580     0.509       333\n\navg / total      0.968     0.963     0.965     10000\n\n\n\n\n\nLogistic Regression with scikit-learn\n\nregr = skl_lm.LogisticRegression()\nregr.fit(X, y)\n\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)\n\n\n\npred = regr.predict(X)\ncm_log = confusion_matrix(y, pred)\ncm_df = pd.DataFrame(cm_log.T, index=regr.classes_,\n                     columns=regr.classes_)\ncm_df.index.name = 'Predicted'\ncm_df.columns.name = 'True'\nprint(cm_df)\n\nTrue          0    1\nPredicted           \n0          9664  333\n1             3    0\n\n\n\npred_p = regr.predict_proba(X)\ncm_df = pd.DataFrame({'True': y, 'Pred': pred_p[:,1] &gt; .25})\ncm_df.Pred.replace(to_replace={True:'Yes', False:'No'}, inplace=True)\nprint(cm_df.groupby(['True', 'Pred']).size().unstack('True').T)\nprint(classification_report(y, pred_p[:,1] &gt; .25))\n\nPred    No  Yes\nTrue           \n0     9371  296\n1      306   27\n             precision    recall  f1-score   support\n\n          0       0.97      0.97      0.97      9667\n          1       0.08      0.08      0.08       333\n\navg / total       0.94      0.94      0.94     10000\n\n\n\n\n# Remove the 'income' predictor\nX = df[['balance', 'student2']].as_matrix()\ny = df.default2.as_matrix()\n\nregr.fit(X, y)\n\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)\n\n\n\npred2 = regr.predict(X)\ncm_log = confusion_matrix(y, pred2)\ncm_df = pd.DataFrame(cm_log.T, index=regr.classes_,\n                     columns=regr.classes_)\ncm_df.index.name = 'Predicted'\ncm_df.columns.name = 'True'\nprint(cm_df)\n\nTrue          0    1\nPredicted           \n0          9639  243\n1            28   90\n\n\n\npred_p2 = regr.predict_proba(X)\ncm_df = pd.DataFrame({'True': y, 'Pred': pred_p2[:,1] &gt; .25})\ncm_df.Pred.replace(to_replace={True:'Yes', False:'No'}, inplace=True)\nprint(cm_df.groupby(['True', 'Pred']).size().unstack('True').T)\nprint(classification_report(y, pred_p2[:,1] &gt; .25))\n\nPred    No  Yes\nTrue           \n0     9490  177\n1      155  178\n             precision    recall  f1-score   support\n\n          0       0.98      0.98      0.98      9667\n          1       0.50      0.53      0.52       333\n\navg / total       0.97      0.97      0.97     10000\n\n\n\n\n### ROC Curves \n\n\n# ROC curve of LDA\nfalse_pos_rate1, true_pos_rate1, _ = roc_curve(y, y_prob[:,1])\nroc_auc1 = auc(false_pos_rate1, true_pos_rate1)\n\n# ROC curve of Logistic Regression with 3 predictors\nfalse_pos_rate2, true_pos_rate2, _ = roc_curve(y, pred_p[:,1])\nroc_auc2 = auc(false_pos_rate2, true_pos_rate2)\n\n# ROC curve of Logistic Regression with 2 predictors\nfalse_pos_rate3, true_pos_rate3, _ = roc_curve(y, pred_p2[:,1])\nroc_auc3 = auc(false_pos_rate3, true_pos_rate3)\n\nfig, (ax1,ax2) = plt.subplots(1, 2, figsize=(14,6))\nax1.plot(false_pos_rate1, true_pos_rate1, label='LDA ROC curve (area = %0.2f)' % roc_auc1, color='b')\nax1.plot(false_pos_rate2, true_pos_rate2, label='Logistic (p=3) ROC curve (area = %0.2f)' % roc_auc2, color='r')\nax1.set_title('Default Dataset')\n\nax2.plot(false_pos_rate1, true_pos_rate1, label='LDA ROC curve (area = %0.2f)' % roc_auc1, color='b')\nax2.plot(false_pos_rate3, true_pos_rate3, label='Logistic (p=2) ROC curve (area = %0.2f)' % roc_auc3, color='r')\nax2.set_title('Default Data')\n\nfor ax in fig.axes:\n    ax.plot([0, 1], [0, 1], 'k--')\n    ax.set_xlim([-0.05, 1.0])\n    ax.set_ylim([0.0, 1.05])\n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    ax.legend(loc=\"lower right\")\n\n\n\n\n\n\n\n\nThe left figure shows the ROC curves for LDA and 3-dimensional logistic regression (default ~ balance + income + student[Yes]). The right figure shows the LDA and 2-variable logistic regression (default ~ balance + student[Yes]). Clearly dropping a bad predictor (income) leads to significant improvements of logistic regression in terms of ROC curves. It is interesting that LDA, as an inverse regression method, seems not much influenced by the bad predictor."
  },
  {
    "objectID": "teaching/FINA6900A.html",
    "href": "teaching/FINA6900A.html",
    "title": "FINA6900A: Machine Learning for Economic Analysis",
    "section": "",
    "text": "Instructor: Zhuang Chu\nTime and Place: Wednesday 19:30-22:20pm, LSK4063\nTeaching Assistant: TBD\n\n\nThis course offers a comprehensive exploration of the fundamental concepts and underlying principles of artificial intelligence (AI). It delves into the core principles of machine learning and provides valuable insights through case studies of relevant technologies. By providing opportunities for hands-on experimentation with machine learning applications, the course aims to inspire students to devise innovative approaches to address real-life problems in fintech using readily-available AI technologies.\nPrerequisite: Some preliminary course on (statistical) machine learning, applied statistics, and deep learning will be helpful."
  },
  {
    "objectID": "teaching/FINA6900A.html#course-information",
    "href": "teaching/FINA6900A.html#course-information",
    "title": "FINA6900A: Machine Learning for Economic Analysis",
    "section": "",
    "text": "Instructor: Zhuang Chu\nTime and Place: Wednesday 19:30-22:20pm, LSK4063\nTeaching Assistant: TBD\n\n\nThis course offers a comprehensive exploration of the fundamental concepts and underlying principles of artificial intelligence (AI). It delves into the core principles of machine learning and provides valuable insights through case studies of relevant technologies. By providing opportunities for hands-on experimentation with machine learning applications, the course aims to inspire students to devise innovative approaches to address real-life problems in fintech using readily-available AI technologies.\nPrerequisite: Some preliminary course on (statistical) machine learning, applied statistics, and deep learning will be helpful."
  },
  {
    "objectID": "teaching/FINA6900A.html#reference-materials",
    "href": "teaching/FINA6900A.html#reference-materials",
    "title": "FINA6900A: Machine Learning for Economic Analysis",
    "section": "Reference Materials",
    "text": "Reference Materials\n\nTextbooks\nAn Introduction to Statistical Learning, with applications in R / Python. By James, Witten, Hastie, and Tibshirani\n\nISLR-python, By Jordi Warmenhoven.\nISLR-Python: Labs and Applied, by Matt Caudill.\n\nManning: Deep Learning with Python, by Francois Chollet\n\n[GitHub source in Python 3.6 and Keras 2.0.8]\n\nMIT: Deep Learning, by Ian Goodfellow, Yoshua Bengio, and Aaron Courville\n\n\nTutorials: preparation for beginners\n\nPython and Machine Learning:\n\nPython Programming for Economics and Finance, by Thomas J. Sargent and John Stachurski\nPython-Numpy Tutorials by Justin Johnson\nscikit-learn Tutorials: An Introduction of Machine Learning in Python\n\nDeep Learning Frameworks:\n\nPyTorch Tutorials\nTensorflow Tutorials"
  },
  {
    "objectID": "teaching/FINA6900A.html#schedule",
    "href": "teaching/FINA6900A.html#schedule",
    "title": "FINA6900A: Machine Learning for Economic Analysis",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nDate\nTopic\nInstructor\n\n\n\n\nSep 6\nLecture 01: Overview and History of Artificial Intelligence in Fintech\nZhuang Chu\n\n\nSep 13\nLecture 02: Supervised Learning: Linear Regression and Classification\n\n[ Reference ]\nTo view .ipynb files below, you may try [ Jupyter NBViewer]\nLinear Regression Python Notebook [ FINA6900A_Regression.ipynb ]\nLinear Classification Python Notebook [ FINA6900A_Classification.ipynb ]\n\nZhuang Chu\n\n\nSep 20\nLecture 03: Model Assessment and Selection: Subset, Ridge, Lasso, and PCR\nZhuang Chu\n\n\nSep 27\nLecture 04: Decision Tree, Bagging, Random Forests and Boosting\nZhuang Chu\n\n\nOct 4\nNo Class (National Holiday)\n-\n\n\nOct 11\nLecture 05: Support Vector Machines\nZhuang Chu\n\n\nOct 18\nLecture 06: An Introduction to Convolutional Neural Networks\nZhuang Chu\n\n\nOct 25\nLecture 07: Other nonlinear models moving beyond linearity\nZhuang Chu\n\n\nNov 1\nLecture 08: An Introduction to Recurrent Neural Networks (RNN) and Long Short Term Memory (LSTM)\nZhuang Chu\n\n\nNov 8\nLecture 09: Attention and Transformers\nZhuang Chu\n\n\nNov 15\nLecture 10: Transformer and Applications\nZhuang Chu\n\n\nNov 22\nLecture 11: An Introduction to Reinforcement Learning with Applications in Quantitative Finance\nZhuang Chu\n\n\nNov 29\nLecture 12: Project Presentation\nZhuang Chu"
  },
  {
    "objectID": "teaching/FINA6900A.html#course-project",
    "href": "teaching/FINA6900A.html#course-project",
    "title": "FINA6900A: Machine Learning for Economic Analysis",
    "section": "Course Project",
    "text": "Course Project\n\nProject 1: Warm-up Project\nKaggle: Home Credit Default Risk competition\n\n\nProject 2: Paper Replication Study\nAsset Pricing paper replication or Reimaging paper replication\n\nShihao Gu, Bryan Kelly and Dacheng Xiu - “Empirical Asset Pricing via Machine Learning”\nJingwen Jiang, Bryan Kelly and Dacheng Xiu - “(Re-)Imag(in)ing Price Trends”\n\n\n\nProject 3: Final Project\nOptions include Kaggle contests, Cryptocurrency Trading Project, or Large Language Models with Financial Analysis"
  },
  {
    "objectID": "teaching/FINA6900A.html#acknowlegements",
    "href": "teaching/FINA6900A.html#acknowlegements",
    "title": "FINA6900A: Machine Learning for Economic Analysis",
    "section": "Acknowlegements",
    "text": "Acknowlegements\nThis course was inspired by and/or uses reserouces from the following courses:\nMIT 6.S191: by Alexander Amini and Ava Amini, MIT\nCS229: Machine Learning by John Cho, Stanford University\nEconDL by Melissa Dell, Harvard University\nFoundations of Machine Learning by Maximilian Kasy, Oxford University"
  }
]